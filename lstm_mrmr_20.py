# -*- coding: utf-8 -*-
"""lstm_mrmr_20.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19zzT2MGidZkNQLqGk2nNomRCe9K84r4g
"""

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive',force_remount=True)

import pickle

with open('/content/drive/MyDrive/Colab Notebooks/RAW_DEAP_DATASET/s01.dat', 'rb') as file:
    x = pickle.load(file, encoding='latin-1')

print(x)

x['labels'].shape

x['data'].shape

!pip install git+https://github.com/forrestbao/pyeeg.git
import numpy as np
import pyeeg as pe
import pickle as pickle
import pandas as pd
import math

from sklearn import svm
from sklearn.preprocessing import normalize

import os
import time

os.getcwd()
os.chdir('/content/drive/My Drive')

import pathlib
import re
from os.path import join, splitext, sep
from typing import List


def natural_sort(in_list: List[str]) -> List[str]:
    """
    :param in_list: list of strings which have to be sort like human do
    :return: sorted list od strings
    """

    def convert(text):
        return int(text) if text.isdigit() else text.lower()

    def alphanum_key(key):
        return [convert(c) for c in re.split('([0-9]+)', key)]

    return sorted(in_list, key=alphanum_key)


def convert_path(path: str) -> str:
    """
    :param path: path string
    :return: os friendly path
    """
    if isinstance(path, pathlib.PurePath):
        return path
    else:
        path_sep = "/" if "/" in path else "\\"
        new_path = join(*path.split(path_sep))
        if splitext(path)[1] is not None and not (
                path.endswith("/") or path.endswith("\\")):
            new_path = new_path + sep
        if path.startswith(path_sep):
            return sep + new_path
        else:
            return new_path


def get_project_root() -> pathlib.Path:
    return pathlib.Path('/content/drive/MyDrive/lstm_mrmr_20.ipynb').parent


def delete_leading_zero(num):
    if not num.startswith("0"):
        return num
    else:
        return delete_leading_zero(num[1:])

from pathlib import Path
ROOT_PATH = get_project_root()
RAW_DATA_PATH = Path(ROOT_PATH, "RAW_DEAP_DATASET")
PREPROCESSED_DATA_PATH = Path(ROOT_PATH, "PREPROCESSED_DEAP_DATA")

import os
import pickle
import re
from os import listdir
from os.path import exists, isdir
from pathlib import Path

from tqdm import tqdm


class LoadData:

    def __init__(self, path):
        self.path = convert_path(path)
        self.__validate_path(path)

    @staticmethod
    def __validate_path(path: str) -> bool:
        try:
            if not isinstance(path, str) or not path:
                return False
            else:
                return exists(path)

        except TypeError:
            return False

    def yield_raw_data(self):
        filenames = None
        if isdir(self.path):
            filenames = next(os.walk(self.path))[2]
        else:
            os.mkdir(self.path)
            exit("Folder doesn't exist")
        filenames = natural_sort(filenames)

        if len(filenames) >= 1:
            pbar = tqdm(filenames, position=0)
            for file in pbar:
                if file.endswith(".dat"):
                    pbar.set_description("Reading %s" % file)
                    with open(Path(self.path, file), 'rb') as f:
                        yield file, pickle.load(f,encoding='latin-1')

    def yield_csv_data(self, label=False):
        if isdir(self.path):
            root_path, dirs = next(os.walk(self.path))[:2]
            for dir in natural_sort(dirs):
                filenames = listdir(Path(root_path, dir))
                if len(filenames) >= 1:
                    if label:
                        filenames = [string for string in filenames if "label" in string]
                    else:
                        filenames = [string for string in filenames if "Trial" in string]
                    filenames = natural_sort(filenames)
                    pbar = tqdm(filenames, position=0)
                    for file in pbar:
                        pbar.set_description("Reading %s" % file)
                        with open(Path(root_path, dir, file)) as f:
                            _file = file.split(".")[0].split("_")
                            participant_number, trial_number = _file[::len(_file) - 1]
                            nums = re.findall(r"\d+", participant_number)
                            if nums:
                                participant_number = nums[0]
                            yield f, participant_number, trial_number
        else:
            print("Folder doesn't exist")
            exit(0)

DEAP_ELECTRODES = ["Fp1", "AF3", "F3", "F7", "FC5", "FC1", "C3", "T7", "CP5", "CP1", "P3", "P7", "PO3", "O1", "Oz",
                   "Pz", "Fp2", "AF4", "Fz", "F4", "F8", "FC6", "FC2", "Cz", "C4", "T8", "CP6", "CP2", "P4", "P8",
                   "PO4", "O2"]

FREQUENCIES = ["Theta", "Alpha", "LowerBeta", "UpperBeta", "Gamma"]

from pathlib import Path

import numpy as np
import pyeeg as pe




def fft_processing(subject, filename, channels, band, window_size, step_size, sample_rate, overwrite, fs=False):
    print(1)
    save_path = PREPROCESSED_DATA_PATH
    p_num = delete_leading_zero(filename.split(".")[0][1:])
    save_file_path = Path(save_path, f"Participant_{p_num}.npy")
    if not save_file_path.exists() or overwrite:
        meta = []
        for i in range(0, 40):
            # loop over 0-39 trails
            data = subject["data"][i]
            # Arousal and Valence
            labels = subject["labels"][i][:2]
            start = 0

            while start + window_size < data.shape[1]:
                meta_array = []
                meta_data = []  # meta vector for analysis
                for j in channels:
                    # Slice raw data over 2 sec, at interval of 0.125 sec
                    x = data[j][start: start + window_size]
                    # FFT over 2 sec of channel j, in seq of theta, alpha, low beta, high beta, gamma
                    y = pe.bin_power(x, band, sample_rate)
                    if (fs):
                        meta_data.append(np.array(y[0]))
                    else:
                        meta_data = meta_data + list(y[0])

                meta_array.append(np.array(meta_data))
                label_bin = np.array(labels >= 5).astype(int)
                meta_array.append(label_bin)

                meta.append(np.array(meta_array, dtype=object))
                start = start + step_size

        meta = np.array(meta)
        if not save_path.exists():
            save_path.mkdir(exist_ok=True)

        np.save(save_file_path, meta, allow_pickle=True, fix_imports=True)

load_data = LoadData(RAW_DATA_PATH)

for filename, data in load_data.yield_raw_data():

    fft_processing(subject=data,
                       filename=filename,
                       channels=range(len(DEAP_ELECTRODES)),
                       band=[4, 8, 12, 16, 25, 45],
                       window_size=256,
                       step_size=16,
                       sample_rate=128,
                       overwrite=True,
                       fs=True)

import numpy as np

# Load the .npy file with allow_pickle=True
data = np.load('/content/drive/MyDrive/PREPROCESSED_DEAP_DATA/Participant_1.npy', allow_pickle=True)

# Print the data
print(data)

data.shape

!pip install mrmr_selection

SAVE_MRMR_CHANNELS_PATH = Path(ROOT_PATH,  "SAVED_TRAINED_MRMR_CHANNELS")
FINAL_DATASET_PATH_MRMR = Path(ROOT_PATH, "FINAL_DEAP_DATASET_MRMR")
FINAL_DATASET_PATH = Path(ROOT_PATH, "FINAL_DEAP_DATASET")

classify_type=input('valence or Arousal:')

from pathlib import Path

import numpy as np

import pandas as pd
from mrmr import mrmr_classif
def use_mrmr(participant_list=range(1, 33), components=20, classify_type: str = "Arousal"):
    print(f"Run MRMR channel selection method with {classify_type} to select {components} channels")

    num_channel = 32
    num_frequencies = 5



    if classify_type.lower() == "arousal":
      save_path_label_training = Path(FINAL_DATASET_PATH_MRMR, "label_training_arousal.npy")
      save_path_label_testing = Path(FINAL_DATASET_PATH_MRMR, "label_testing_arousal.npy")
      save_path_data_training = Path(FINAL_DATASET_PATH_MRMR, "data_training_arousal.npy")

      save_path_data_testing = Path(FINAL_DATASET_PATH_MRMR, "data_testing_arousal.npy")
    else:
      save_path_label_training = Path(FINAL_DATASET_PATH_MRMR, "label_training_valence.npy")
      save_path_label_testing = Path(FINAL_DATASET_PATH_MRMR, "label_testing_valence.npy")
      save_path_data_training = Path(FINAL_DATASET_PATH_MRMR, "data_training_valence.npy")

      save_path_data_testing = Path(FINAL_DATASET_PATH_MRMR, "data_testing_valence.npy")



    FINAL_DATASET_PATH.mkdir(exist_ok=True)
    SAVE_MRMR_CHANNELS_PATH.mkdir(exist_ok=True)

    x_train = []
    y_train = []
    x_test = []
    y_test = []
    backup = []
    for participant in participant_list:
        filename = f"Participant_{participant}.npy"
        with open(Path(PREPROCESSED_DATA_PATH, filename), "rb") as file:
            sub = np.load(file, allow_pickle=True)
            data = []
            label = []
            for i in range(0, sub.shape[0]):
                data.append(np.array(sub[i][0]))
                label.append(np.array(sub[i][1]))
            data = np.array(data)
            label = np.array(label)

        x = data.transpose((1, 0, 2)).reshape(num_channel, -1).transpose((1, 0))
        if classify_type.lower() == "arousal":
            y = np.repeat(label[:, 0], num_frequencies)
        else:
            y = np.repeat(label[:, 1], num_frequencies)

        x = pd.DataFrame(x)
        y = pd.Series(y)

        mrmr_x_idx = mrmr_classif(X=x, y=y, K=components)

        backup.append([participant, mrmr_x_idx, len(mrmr_x_idx)])

        data_new = x[mrmr_x_idx].to_numpy()

        z = []
        for i in data_new.transpose(1, 0):
            z.append(i.reshape(-1, num_frequencies))

        zx = np.array(z).transpose((1, 0, 2))

        for i in range(0, zx.shape[0]):
            if i % 4 == 0:
                x_test.append(zx[i].reshape(zx.shape[1] * zx.shape[2], ))
                y_test.append(label[i])
            else:
                x_train.append(zx[i].reshape(zx.shape[1] * zx.shape[2], ))
                y_train.append(label[i])

    pd.DataFrame(backup, columns=["participant", "channels", "n_channels"], index=None).to_csv(
        Path(SAVE_MRMR_CHANNELS_PATH, f"mrmr_channels_{classify_type}.csv"), index=False)
    print(len(x_train))
    print(len(y_train))

    print(len(x_test))
    print(len(y_test))
    np.save(save_path_data_training, np.array(x_train), allow_pickle=True, fix_imports=True)
    np.save(save_path_label_training, np.array(y_train), allow_pickle=True, fix_imports=True)

    np.save(save_path_data_testing, np.array(x_test), allow_pickle=True, fix_imports=True)
    np.save(save_path_label_testing, np.array(y_test), allow_pickle=True, fix_imports=True)

    print("Dataset has been transformed with mRMR")

use_mrmr(participant_list=range(1, 33), components=20, classify_type=classify_type)

from pathlib import Path

import numpy as np
from sklearn.preprocessing import normalize, StandardScaler
from keras.utils import to_categorical
def prepare_dataset(label_type: str = "Arousal", mrmr: bool = True):
    if mrmr:
        data_path = FINAL_DATASET_PATH_MRMR

    else:
        data_path = FINAL_DATASET_PATH


    if classify_type.lower() == "arousal":
      y_train = np.load(str(Path(data_path, "label_training_arousal.npy")))
      y_test = np.load(str(Path(data_path, "label_testing_arousal.npy")))
      x_train = np.load(str(Path(data_path, "data_training_arousal.npy")))
      x_test = np.load(str(Path(data_path, "data_testing_arousal.npy")))
    else:
      y_train = np.load(str(Path(data_path, "label_training_valence.npy")))
      y_test = np.load(str(Path(data_path, "label_testing_valence.npy")))
      x_train = np.load(str(Path(data_path, "data_training_valence.npy")))
      x_test = np.load(str(Path(data_path, "data_testing_valence.npy")))


    x_train = normalize(x_train)
    y_train_arousal = np.ravel(y_train[:, [0]])
    y_train_valence = np.ravel(y_train[:, [1]])
    y_train_arousal = to_categorical(y_train_arousal)
    y_train_valence = to_categorical(y_train_valence)
    x_train = np.array(x_train[:])




    x_test = normalize(x_test)
    y_test_arousal = np.ravel(y_test[:, [0]])
    y_test_valence = np.ravel(y_test[:, [1]])
    y_test_arousal = to_categorical(y_test_arousal)
    y_test_valence = to_categorical(y_test_valence)
    x_test = np.array(x_test[:])

    scaler = StandardScaler()
    x_train = scaler.fit_transform(x_train)
    x_test = scaler.fit_transform(x_test)

    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)
    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)

    if label_type.lower() == "arousal":
        return x_train, y_train_arousal, x_test, y_test_arousal
    else:
        return x_train, y_train_valence, x_test, y_test_valence

x_train, y_train, x_test, y_test = prepare_dataset(classify_type,False)

print("Training: ", x_train.shape, y_train.shape)
print("Test: ", x_test.shape, y_test.shape)

import os

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
from tensorflow.keras.layers import Bidirectional, LSTM, Dropout, Dense, Activation
from tensorflow.keras.losses import categorical_crossentropy
from tensorflow.keras.models import Sequential


def build_model(input_shape):
    model = Sequential()

    model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=input_shape))
    model.add(Dropout(0.6))

    model.add(LSTM(units=256, return_sequences=True))
    model.add(Dropout(0.6))

    model.add(LSTM(units=64, return_sequences=True))
    model.add(Dropout(0.6))

    model.add(LSTM(units=64, return_sequences=True))
    model.add(Dropout(0.4))

    model.add(LSTM(units=32))
    model.add(Dropout(0.4))

    model.add(Dense(units=16))
    model.add(Activation('relu'))

    model.add(Dense(units=2))
    model.add(Activation("softmax"))
    model.compile(optimizer="adam", loss=categorical_crossentropy, metrics=["accuracy"])
    print(model.summary())
    return model


def training(y_train, y_test, x_train, x_test, epochs):
    model = build_model(input_shape=(x_train.shape[1], 1))

    history = model.fit(x_train, y_train, epochs=epochs, batch_size=256, verbose=1, validation_data=(x_test, y_test))

    return history, model

h, m = training(y_train, y_test, x_train, x_test, 50)

loaded.fit(x_train, y_train, epochs=20, batch_size=256, verbose=1, validation_data=(x_test, y_test))

loaded.save('lstm_20_arousal.h5')

from tensorflow.keras.models import load_model
loaded_valence=load_model('lstm_20_valence.h5')
loaded_arousal=load_model('lstm_20_arousal.h5')

score = loaded_arousal.evaluate(x_test,y_test, verbose=1)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

score = loaded_valence.evaluate(x_test,y_test, verbose=1)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

if classify_type == "arousal":
  y_pred_arousal=loaded_arousal.predict(x_test)
else:
  y_pred_valence=loaded_valence.predict(x_test)

y_pred_valence=np.argmax(y_pred_valence, axis=1)

y_pred_arousal=np.argmax(y_pred_arousal, axis=1)

y_pred_valence.shape

for i in range(len(y_pred_arousal)):
  a=y_pred_valence[i]
  b=y_pred_arousal[i]

  if a==0 and b==0:
    print('3rd Quadrant')
  elif (a==0 and b==1):
    print('2nd Quadrant')
  elif (a==1 and b==0):
    print('4th Quadrant')
  else:
    print('1st Quadrant')